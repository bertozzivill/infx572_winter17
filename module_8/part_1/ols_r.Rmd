---
title: "OLS in R"
output: html_document
---

Refresher: We're wondering how `yrs.since.phd` is associated with `salary` in the `Salaries` dataset from the `car` library. If you haven't already. load the necessary libraries, load this dataset, and convert it to a `data.table`. Also load the `ggplot2` library, we'll need it later.

```{r, echo=F}
library(data.table)
library(ggplot2)
library(car)

data("Salaries")
Salaries <- data.table(Salaries)
```

## Running a Regression

Let's take a look at our dataset:
```{r}
print(Salaries)
```
<br>
Again, we want to regress `yrs.since.phd` on `salary`. The function that you use to run a linear regression is called `lm()`, for "linear model". The two main arguments that `lm()` needs are a `formula` argument *specifying* the regression, and a `data` argument telling it what dataset to work with. 

### Specifying the model with `formula`

Remember formulas? We used them to reshape datasets and to facet plots. They're the ones with the `~`. Also, remember how last lecture we wrote that equation that *specified* our formula? The one that had the variables we wanted to include, but didn't have values for $\beta_0$ or $\beta_1$ yet:

$$salary = \beta_0 + \beta_1yrs.since.phd + \varepsilon$$

In R, we use `formula`s to specify our model exactly the same way. A regression formula in R will always be of the format:
<br>
`y_variable ~ x_variable_1 + x_variable_2 + x_variable_3...`
<br>

Where `y_variable`, `x_variable_1`, etc. are all column names in your dataset. **There will never be more than one variable on the left side of the `~`**: You can only regress on one outcome variable at once. Today, we're only going to have one x variable to the right side of the `~`, but in theory you could have many: we'll get to that next week. 

Again, remember, **outcome variable goes to the left, predictor variable(s) go to the right**. 

The formula for the regression we want to run, then, is `salary ~ yrs.since.phd`. 

Notice that this is considerably more parsimonious than the big honking equation we wrote above-- it doesn't have any $\beta$s in it, and we haven't written out either our error term $\varepsilon$ or our intercept term $\beta_0$. This is because a good regression will *always* include an intercept, and *must* by definition allow some room for error, so both of these terms are generated automatically by the `lm` function. We'll still get an output for the estimated value of our $\beta_0$ even though we didn't write out an intercept term. 

Similarly, we don't have to write out any $\beta$s explicity because there's the assumption that, if you're including something in the model, you *must* want to associate a $\beta$ with it. So, what the `lm` function interprets when it sees `salary ~ yrs.since.phd` is "Run a linear regression with an outcome variable of `salary` and a predictor variable of `yrs.since.phd`. Return $\beta$ values both for `yrs.since.phd` and for the intercept."

### Pointing to data with `data`

The `data` argument to `lm` is easy, it's just the name of your dataset.

### Putting it Together

It's finally time to run our first regression. The `formula` argument comes before the `data` one, so we write:

```{r}
regression_output <- lm(salary ~ yrs.since.phd, data=Salaries)
print(regression_output)
```
<br> 
And there we have it! This linear regression predicts an intercept value ($\beta_0$) of \$91,718.70, and a coefficient on `yrs.since.phd` (i.e. a $\beta_1$) of \$985.30. Remember how to interpret these regression outputs from our first regression lecture?

**"A professor with zero years since his/her Ph.D is expected to make \$91,718.70, and every additional year since Ph.D adds \$985.30 to that profesor's expected salary."**

Just printing out the output of a regression gives you the bare minimum of information about it: what regression you ran, and what your $\beta$ values were. You can get much richer information by calling `summary` on your regression output, but it can be kind of hard to interpret. Let's review it below. 

## Interpreting Regression Output

Running `print(regression_output)` gave us results that were easy to interpret, but which didn't contain much information. Let's try this instead:

```{r}
summary(regression_output)
```
<br>
Whoa! That's a lot more information! Let's walk through each section at a time. 

### `Call`
This is the same as when you use your `print` command-- just an echo of specifically what code you ran to generate this regression output. Useful for reference. 

### `Residuals`
Remember what a residual is? Its' the difference between a given data point and the predicted regression line. If you do `regression_output$residuals`, you'll see that the residuals are a *vector* with as many entries as there are rows in the dataset, specifying the residual for each data point in turn. The summarized output just shows you some range values for your residuals. Residuals are calculated by subtracting the value of the *regression output* from the value of the *data point*. So, in the regression we just ran, our predicted line is at maximum \$84,171 *higher* than the data points it predicts for, and at minimum \$102,383 *lower* than the data points it predicts for. 

### `Coefficients`
This is the main section of interest for us-- it tells us about our $\beta$ values, their standard errors, and their p-values. The leftmost column is a list of all the variables that were included in the regression. Again, notice that even though we didn't specify an Intercept term, one was generated for us automatically. 

The `Estimate` column gives us the estimated $\beta$, or coefficient, value for that variable. This is analogous to the simple output we see when we run `print(regression_output)`. 

The `Std. Error` column gives us the estimated standard errors for each variable. Thinking back to the distributions we made last time, we see that the standard error for the `yrs.since.phd` variable is pretty small, meaning that there's not much uncertainty about the value of this coefficient. 

The `t value` column is used in the calculation of p-values, beyond the scope of this class.

The confusingly-named `Pr(>|t|)` column gives us p-values for each variable. The asterisks next to them, as explained in the `Signif. codes` section underneath the table, indicates how statistically significant each coefficient is. For very small numbers, p-values are specified in *scientific notation*. So `2e-16` means $2*10^{-16}$, or $0.0000000000000002$. That's really, really small! Way below $0.05$, the classic cutoff for statistical significance. You'll see the `<2e-16` p-value often: it's what R uses when the p-value is to small to represent any other way. 

### Others
The final three lines of this output give us some useful statistics for the regression as a whole. The only one of these I want you to pay attention to is the Adjusted R-squared value. R-squared, along with p-value, is another common statistic for assessing the goodness-of-fit of your model. the R-squared value will tell you *how much variation in your outcome variable can be predicted by your predictor variable*. If the data were a straight line, the R-squared value would be 1. If the data were random noise, R-squared would be close to zero (or could, in some cases, even be negative). Here we have an adjusted R-squared of 0.1737, meaning that only about 17% of the variance in `salary` can be explained by `yrs.since.phd`. 

### Some Things to Note About Regression Outputs:

* The last line of the regression output specifies a p-value for the regression as a whole. **IGNORE THIS VALUE**. We're only interested in p-values in the context of specific variables. 
* The regression gives standard error and p-values for the Intercept term, but we rarely or never report these, as they don't have much meaning. For the intercept, we're really only interested in the `Estimate` value. For all other variables, however, you always want to report the mean ("Estimate") value, the SE, *and* the p-value.
* The `Estimate` values are always in units of whatever the *outcome* variable is in units of. So everything in the `Estimate` column is in units of dollars, because the outcome variable `salary` is in units of dollars.


## Predicting From Regression Outputs

Remember our motivations for statistical analysis: inference and prediction. The regression outputs as they stand tell us everything we need to know about inference, but what about prediction? How do we guess what a person's salary is, based on their years since Ph.D?  For that, we use the `predict` function. 

`predict` by default takes only one argument-- the output of the linear regression you run. So, if we ran `predict(regression_output)` we would get a vector of predicted salary values for each professor in our dataset. It's common to save this as a column in the dataset itself:

```{r}
Salaries[, predicted_salary:= predict(regression_output)]
```
<br>
See? Now we have a new column in our dataset specifying our *predicted* salaries. All that the `predict` function does is the arithmetic we did by hand last lecture: If $\beta_0$ is \$91,718.70, $\beta_1$ is \$985.30, and the first professor in the dataset has 19 years since his Ph.D, then his predicted salary is $91,718.70 + 985.30*19$, or \$110,440. 

If, instead, we want to predict salaries for *new* professors who weren't included in the dataset, we can use the argument `newdata` to `predict` to accomplish that. First, we need to make a new dataset of professors for whom we want to predict salaries. It *must* include columns with *exactly the same names* as all our *predictor* variables. So, in this case, it *must* have a column called "yrs.since.phd", and any other columns are optional. I'll make a small dataset that just lists some ID value for each professor, and their yrs.since.phd:

```{r}
new_salaries <- data.table(prof.id=1:5,
                           yrs.since.phd=c(13, 25, 3, 27, 9))
print(new_salaries)
```
<br>
Great! Now to predict salaries for these individuals I just need to do:
```{r}
predict(regression_output, newdata=new_salaries)
```
<br>
Or, to add a new column to this dataset:
```{r}
new_salaries[, predicted_salary:= predict(regression_output, newdata=new_salaries)]
```


## Plotting Regression Outputs
In the `ggplot` section, we learned a quick and dirty shortcut to plotting a best-fit line using `geom_smooth`. But if we go through the trouble of actually running a regression, we should plot the actual regression outputs, not the `geom_smooth`-ed line. Now that `predict` has already given us a new column in our dataset, we can accomplish this easily: just replace the `geom_smooth` geom with a `geom_line` plotting the `predicted_salary` column. Pay close attention to where I put which aesthetics:

```{r}
ggplot(Salaries, aes(x=yrs.since.phd)) +
  geom_point(aes(y=salary)) +
  geom_line(aes(y=predicted_salary), color="blue") +
  labs(title="Data and Regression Line for Years Since Ph.D and Salary among Professors",
       x="Years Since Ph.D",
       y="Salary (USD)")
```

## Another Example: Cereal

Let's load and clean the `UScereal` dataset again.

```{r, results='hide'}
# load MASS library and UScereal dataset
library(MASS)
data(UScereal)

# save the names of cereals as an actual row, instead of an index value
UScereal$cereal_name <- rownames(UScereal) 

# convert to a data.table
UScereal <- data.table(UScereal) 

# clean up some column values
UScereal[, shelf:=NULL] # we won't need that column
UScereal[, sodium:= sodium/1000] # this converts the units of the "sodium" column from milligrams to grams
UScereal[, potassium:=potassium/1000] # converts units of the "potassium" column from milligrams to grams 

# rearrange the column order
setcolorder(UScereal, c("cereal_name", "mfr", "vitamins", "calories", "protein", "fat", "sodium", "fibre", "carbo", "sugars", "potassium"))
```
<br>

Say I'm interested in analyzing whether or not more fat per serving of a cereal corresponds to more calories per serving. I want `calories` to be my outcome variable, and `fat` to be my predictor variable. How would I specify my regression? Try it on your own before looking at the answer.

...

...

Ready?

The code to run this regression is: 
```{r}
cereal_output <- lm(calories ~ fat, data=UScereal)
summary(cereal_output)
```

Using the `Salaries` example above as a guide, see if you can determine: 

* What the regression coefficients are for both the intercept and the `fat` variable.
* What the standard error and p-value is for the `fat` variable.
* What those regression coefficients *mean*--see if you can put them in a sentence, like the one in bold above for `Salaries`.
* What the adjusted R-squared is for this regression.


I'll give you the answer to the third question: 

**"A cereal with zero grams of fat is expected to have 117.6 calories,, and every additional gram of fat adds 22.36 to that cereal's expected caloric value."**

Let's plot this regression! Remember, we can create a new column in the dataset with predicted values using `predict`:
```{r, results='hide'}
UScereal[, predicted_calories:= predict(cereal_output)]
```

```{r, eval=F}
UScereal[, predicted_calories:= predict(cereal_output)]

ggplot(UScereal, aes(x=fat)) +
  geom_point(aes(y=calories)) +
  geom_line(aes(y=predicted_calories), color="blue") +
  labs(title = "Data and Regression Line for Fat and Calories among US Breakfast Cereals",
       x="Fat (g per Serving)",
       y="Calories per Serving")
```
<br>

We've just scratched the surface of regressions here-- next time, we'll dig a little deeper into assumptions of linearity and correct model specifications, and learn how to add new variables to regressions.

