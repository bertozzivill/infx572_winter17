---
title: "Logistic Regression"
author: "Amelia Bertozzi-Villa"
date: "February 22, 2017"
output: html_document
---

Logistic regression straddles the boundary between machine learning and statistics-- it's a classification algorithm, but it uses (statistical) regression techniques. Logistic regressions are easy to interpret if you're comfortable with natural logarithms, exponentiation, and odds-- let's review these topic before we get into the details of how logistic regression works. 

## Script for e/log definitions

Definitions: e, exponents, logs


### e
Let's start by defining the number e.

$$ e $$

In mathematics, "e" doesn't represent a letter, it represents a number. Like pi, e is an irrational number-- that is, its decimal representation goes on forever with no repetitions or other patterns-- and also like pi, e is extremely important both in pure and applied mathematics. The first few digits of e are 2.71828. 


### Exponents
Most of you are probably familiar with the idea of exponentiation, or taking a number "to the power" of another number.

$$ b^n $$

Exponentiation requires two values: the "base" value b, and the "exponent" n. This operation means "multiply b by itself n times". 

So, if our base is 10 and our exponent is 2, we multiply 10 by itself twice:

$$ 10^2 = 10*10 = 100 $$ 

Notice that you can have negative exponents, which is the same as dividing by a positive exponent:

$$ 10^{-2}=\frac{1}{10^2} = \frac{1}{100} = 0.01 $$

And, importantly, that you can have non-integer exponents, which it's easiest to calculate using a computer:

$$ 10^{1.5} = 31.62278 $$ 

The base value, b, can also be whatever you want it to. it's very common to have b take the value of e: 

$$ e^x $$

For any exponentiated value b, b^1 is always b and b^0 is always 1:

$$ b^1=b \\b^0 = 1 $$


Exponentiated values have the nice property that addition in the exponents can also be expressed as multiplication in the bases:

$$ e^{a + b} = e^a * e^b $$

Here's a concrete example of this:

$$ 10^{2+3} = 10^5 =  100,000 \\ 10^{2+3} = 10^2 * 10^3 = 100 * 1000 = 100,000 $$ 

### Logarithms

#### Inverse function
An *inverse function* is something that does the opposite of some other function. Let's say we have a funtion f(x) that takes a value x and multiplies it by 2:

$$ f(x) = 2x $$

The *inverse function* of f(x) would be a function that *divides* a value by 2: 

$$ g(x) = x/2 $$

You know that two functions are the inverse of each other when performing them in sequence recovers the original x value. For example, let's say our x value is 4. Let's run f(x) on this value, *then* run g(x) on the output of that function:

$$ f(4) = 2*4 = 8 \\ g(8) = 8/2 = 4$$

you can also see this by nesting the two functions inside each other:

$$ g(f(x)) = \frac{f(x)}{2} = \frac{2x}{2} = x $$

#### Logarithms 
In the same way that x/2 is the inverse function of 2x, a *logarithm* is the inverse function to exponentiation. Like exponentiation, logarithms have a *base*, and only exponential functions and logarithmic functions with the *same base* are inverses of each other. So:

$$ log_{10}(x) $$ 

and 

$$ 10^x $$ 

Are inverse functions to each other, but

$$ log_{10}(x) $$ 

and 

$$ e^x $$ 
are *not* inverse functions to each other. 

When you run a logarithmic function with a certain base b on a number x, it returns *the number you would need to exponentiate b by in order to get x*. For example, log base 10 of 100 is 2:

$$ log_{10}(100) = 2$$ 

What this means is: "You have to raise 10 to the power of 2 in order to get 100"

You can see from this example how the two functions are inverse to each other:

$$ log_{10}(10^2) = log_{10}(100) = 2 $$ 

The function e^x is so common that its inverse function log base e is called the "natural logarithm"-- it's often written shorthand as simply ln(x). 

$$ e^x $$

$$log_e(x) \\ ln(x)$$

Remember: 

$$ ln(e^x) = x $$ 


## Script for odds definitions
Now, let's briefly define some terms associated with probability: *odds* and *odds ratio*

A single *odds* is itself a ratio: the ratio of how likely a thing is to happen, compared to it not happening. If there are "3 to 1 odds" of something happening, it means that it will happen 3 out of 4 times, or that the probability of it happening is 75%. If something has a probability p of happening, the odds are defined as: 

$$ odds = \frac{p}{1-p} $$
If something has a 25% chance of happening, the odds are :

$$ odds = \frac{0.25}{1-0.25} = \frac{0.25}{0.75} = 1/3 $$

that is to say: 1 to 3, or 0.3333333.

An *odds ratio* is a ratio of two odds, and it's a useful way of comparing the odds for two different groups or scenarios. Let's say if you want to know whether men or women are more likely to purchase some product. Let's say the odds of a woman purchasing that product are 1 to 4, or 0.25 (note, this is still and *odds*, not a probability-- it's just being expressed as a decimal instead of a fraction). And let's say the odds of a man purchasing it are 1 to 3, or 0.33333. The *odds ratio* of a man purchasing that product compared to a woman is :

$$ OR_{male} = \frac{male \ odds}{female \ odds} \frac{1/3}{1/4} = \frac{0.3333}{0.25} \approx 1.3333 $$

What the odds ratio means is that a man is 1.3333 times as likely to purchase that product as a woman is-- or, percentage-wise, that a man is 33.33% more likely to purchase it. 

If we want to know the odds ratio of a *woman* purchasing the product instead, we just flip the equation:

$$ OR_{female} = \frac{female \ odds}{male \ odds} \frac{1/4}{1/3} = \frac{0.25}{0.3333} = 0.75 $$

We read this as, "a woman is 0.75 times as likely to purchase the product as a man is", or "a woman is 25% less likely to purchase the product than a man is".

Notice that these two values are not exact mirrors of each other--when you're working with percent changes like this, your reference group matters. Odds ratios can't go below 0, but can go all the way up to infinity on the positive side. 

You can also calculate an odds ratio along a continuous variable, by comparing odds at two different points. Let's think about life expectancy. Say your probability of surviving past age 35 was only 80% in 1895, but rose up to 85% in 1896. The odds would be:

$$ odds_{1895} = \frac{0.8}{1-0.8} = 4 $$
$$ odds_{1896} = \frac{0.85}{1-0.85} = 5.6667 $$

To get the odds ratio of those two years, you would do:

$$ OR = \frac{odds_{1896}}{odds_{1895}} = \frac{5.6667}{4} = 1.417 $$
That is: in 1896, you were 1.417 times as likely to live past age 35 as you were in 1895, or: You were 41.7% more likely to live past 35 in 1896 as you were in 1895. 

One more note: You remember our old friend the natural log, ln()? Let's say for some reason that someone takes the natural log of an odds or odds ratio, and gives that to you instead of the actual odds. This is called a "log odds" (or "log odds ratio", respectively), and it's easy to recover the original value: just take the exponent! 

So, if I gave you a log odds ratio of 0.3483, you could recover the original odds ratio by simply doing:

$$ e^{0.3483} = 1.417 $$ 

This will become relevant later.


## Script for Logistic regression intro/motivation

## Script for example in R



```{r}
# Logistic regression example
library(data.table)
library(ggplot2)

data(mtcars)
mtcars <- data.table(mtcars)

# transform "am" to a categorical variable
mtcars[, type:= ifelse(am==1, "manual", "auto")]

# plot initial results
ggplot(mtcars, aes(x=mpg, y=vs)) +
  geom_point(aes(color=type))


# logistic regression with mpg
logistic_mpg <- glm(vs~mpg, data=mtcars, family=binomial)
summary(logistic_mpg)

# logicstic regression with mpg and type
logistic_mpg_type <- glm(vs~mpg+type, data=mtcars, family=binomial)
summary(logistic_mpg_type)

# predict probability of a straight engine
mtcars[, prob_sengine:=predict(logistic_mpg_type, type="response")]

#plot logistic results
ggplot(mtcars, aes(x=mpg)) +
  geom_point(aes(y=vs, color=type)) +
  geom_line(aes(y=prob_sengine, color=type), size=1)

```
